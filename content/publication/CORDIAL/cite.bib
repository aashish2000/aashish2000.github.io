@ARTICLE{Anantha-Ramakrishnan2025-ce,
  title         = "{CORDIAL}: Can multimodal Large Language Models effectively
                   understand coherence relationships?",
  author        = "Anantha Ramakrishnan, Aashish and Anantha Ramakrishnan,
                   Aadarsh and Dongwon, Lee",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multimodal Large Language Models (MLLMs) are renowned for
                   their superior instruction-following and reasoning
                   capabilities across diverse problem domains. However,
                   existing benchmarks primarily focus on assessing factual and
                   logical correctness in downstream tasks, with limited
                   emphasis on evaluating MLLMs' ability to interpret pragmatic
                   cues and intermodal relationships. To address this gap, we
                   assess the competency of MLLMs in performing Multimodal
                   Discourse Analysis (MDA) using Coherence Relations. Our
                   benchmark, CORDIAL, encompasses a broad spectrum of Coherence
                   Relations across 3 different discourse domains at varying
                   levels of granularity. Through our experiments on 10+ MLLMs
                   employing different prompting strategies, we show that even
                   top models like Gemini 1.5 Pro and GPT-4o fail to match the
                   performance of simple classifier-based baselines. This study
                   emphasizes the need to move beyond similarity-based metrics
                   and adopt a discourse-driven framework for evaluating MLLMs,
                   providing a more nuanced assessment of their capabilities.
                   The benchmark and code are available at:
                   https://github.com/aashish2000/CORDIAL.",
  month         =  feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}