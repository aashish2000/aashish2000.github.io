@article{Anantha_Ramakrishnan2023-zo,
 abstract = {Advancements in Text-to-Image synthesis over recent years
have focused more on improving the quality of generated
samples on datasets with descriptive captions. However,
real-world image-caption pairs present in domains such as
news data do not use simple and directly descriptive
captions. With captions containing information on both the
image content and underlying contextual cues, they become
abstractive in nature. In this paper, we launch ANNA, an
Abstractive News captioNs dAtaset extracted from online news
articles in a variety of different contexts. We explore the
capabilities of current Text-to-Image synthesis models to
generate news domain-specific images using abstractive
captions by benchmarking them on ANNA, in both standard
training and transfer learning settings. The generated images
are judged on the basis of contextual relevance, visual
quality, and perceptual similarity to ground-truth
image-caption pairs. Through our experiments, we show that
techniques such as transfer learning achieve limited success
in understanding abstractive captions but still fail to
consistently learn the relationships between content and
context features.},
 archiveprefix = {arXiv},
 author = {Anantha Ramakrishnan, Aashish and Huang, Sharon X and Lee,
Dongwon},
 journal = {arXiv [cs.CV]},
 month = {January},
 primaryclass = {cs.CV},
 title = {{ANNA}: Abstractive Text-to-Image Synthesis with Filtered
News Captions},
 year = {2023}
}
